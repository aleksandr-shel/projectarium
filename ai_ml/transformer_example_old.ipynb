{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ec097e-2152-4efb-9ee3-b8989ccd28e0",
   "metadata": {
    "id": "89ec097e-2152-4efb-9ee3-b8989ccd28e0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "import random\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "PNcGz4Ji7o4K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNcGz4Ji7o4K",
    "outputId": "95ce5566-d2d8-4855-e8bc-0f8c4eb3dbe0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda9fdf5-9e24-4895-8c17-48f96b42e766",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dda9fdf5-9e24-4895-8c17-48f96b42e766",
    "outputId": "e6378c22-860c-4273-be39-a3d92be8a93f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "578d65e0-fdb8-4a13-9883-a9390a9ee00d",
   "metadata": {
    "id": "578d65e0-fdb8-4a13-9883-a9390a9ee00d"
   },
   "outputs": [],
   "source": [
    "# filepath='./drive/MyDrive/Colab Notebooks/data.csv'\n",
    "filepath='./data/data.csv'\n",
    "data=pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e84a1cbd-448c-44c5-94c7-ea1f5f781fbe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e84a1cbd-448c-44c5-94c7-ea1f5f781fbe",
    "outputId": "f8a7a232-82fc-4627-dc49-1850d14153de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Unnamed: 0           50000 non-null  int64  \n",
      " 1   idx                  50000 non-null  int64  \n",
      " 2   original             50000 non-null  object \n",
      " 3   en                   50000 non-null  object \n",
      " 4   ru                   50000 non-null  object \n",
      " 5   chrf_sim             50000 non-null  float64\n",
      " 6   labse_sim            50000 non-null  float64\n",
      " 7   forward_entailment   50000 non-null  float64\n",
      " 8   backward_entailment  50000 non-null  float64\n",
      " 9   p_good               50000 non-null  float64\n",
      "dtypes: float64(5), int64(2), object(3)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cab2bc7-4ebc-43bc-aba6-44bf029f73b1",
   "metadata": {
    "id": "3cab2bc7-4ebc-43bc-aba6-44bf029f73b1"
   },
   "outputs": [],
   "source": [
    "data['en'].to_csv('en.txt', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3571d578-d645-4c9b-9ac6-3b28f16cf5d5",
   "metadata": {
    "id": "3571d578-d645-4c9b-9ac6-3b28f16cf5d5"
   },
   "outputs": [],
   "source": [
    "data['ru'].to_csv('ru.txt', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f92f7bf-ba60-42dd-bf62-a41b07cc199a",
   "metadata": {
    "id": "0f92f7bf-ba60-42dd-bf62-a41b07cc199a"
   },
   "outputs": [],
   "source": [
    "UNK_IDX, BOS_IDX, EOS_IDX, PAD_IDX = 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d1406a4-d386-42c3-ba7d-ded82115807f",
   "metadata": {
    "id": "7d1406a4-d386-42c3-ba7d-ded82115807f"
   },
   "outputs": [],
   "source": [
    "vocab_src = 20000\n",
    "vocab_tgt = 20000\n",
    "options1 = dict(\n",
    "  # input spec\n",
    "  input=\"en.txt\",\n",
    "  input_format=\"text\",\n",
    "  # output spec\n",
    "  model_prefix=\"en\", # output filename prefix\n",
    "  # algorithm spec\n",
    "  # BPE alg\n",
    "  model_type=\"bpe\",\n",
    "  vocab_size=vocab_src,\n",
    "  # normalization\n",
    "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=200000000, # max number of training sentences\n",
    "  max_sentence_length=4192, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment\n",
    "  character_coverage=0.99995,\n",
    "  byte_fallback=True,\n",
    "  # merge rules\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special tokens\n",
    "  unk_id=UNK_IDX, # the UNK token MUST exist\n",
    "  bos_id=BOS_IDX, # the others are optional, set to -1 to turn off\n",
    "  eos_id=EOS_IDX,\n",
    "  pad_id=PAD_IDX,\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(), # use ~all system resources\n",
    ")\n",
    "\n",
    "options2 = dict(\n",
    "  # input spec\n",
    "  input=\"ru.txt\",\n",
    "  input_format=\"text\",\n",
    "  # output spec\n",
    "  model_prefix=\"ru\", # output filename prefix\n",
    "  # algorithm spec\n",
    "  # BPE alg\n",
    "  model_type=\"bpe\",\n",
    "  vocab_size=vocab_tgt,\n",
    "  # normalization\n",
    "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=200000000, # max number of training sentences\n",
    "  max_sentence_length=4192, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment\n",
    "  character_coverage=0.99995,\n",
    "  byte_fallback=True,\n",
    "  # merge rules\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special tokens\n",
    "  unk_id=UNK_IDX, # the UNK token MUST exist\n",
    "  bos_id=BOS_IDX, # the others are optional, set to -1 to turn off\n",
    "  eos_id=EOS_IDX,\n",
    "  pad_id=PAD_IDX,\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(), # use ~all system resources\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options1)\n",
    "spm.SentencePieceTrainer.train(**options2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee6b09a0-aff4-4b19-8108-7aeb046ba9e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee6b09a0-aff4-4b19-8108-7aeb046ba9e0",
    "outputId": "02218ab7-e583-4ddd-9290-517633717fb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_src = spm.SentencePieceProcessor()\n",
    "sp_tgt = spm.SentencePieceProcessor()\n",
    "sp_src.load('en.model')\n",
    "sp_tgt.load('ru.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "344dd935-d822-4556-b911-256be02d5ed9",
   "metadata": {
    "id": "344dd935-d822-4556-b911-256be02d5ed9"
   },
   "outputs": [],
   "source": [
    "def encode(text, sp):\n",
    "    return torch.tensor(sp.encode(text)).to(device)\n",
    "def process_text(text, sp):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(sp.encode(text)),\n",
    "                      torch.tensor([EOS_IDX]))).to(device)\n",
    "def to_device(t):\n",
    "    return t.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d325d567-450b-4880-9e8a-9ddb451c141f",
   "metadata": {
    "id": "d325d567-450b-4880-9e8a-9ddb451c141f"
   },
   "outputs": [],
   "source": [
    "data_ = data[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0dadfe3-7e3e-46ce-80b9-abe5054c3254",
   "metadata": {
    "id": "b0dadfe3-7e3e-46ce-80b9-abe5054c3254"
   },
   "outputs": [],
   "source": [
    "\n",
    "src = data_['en'].apply(process_text, sp=sp_src)\n",
    "tgt = data_['ru'].apply(process_text, sp=sp_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdee6492-2dfe-44c0-be45-3c8db1492e78",
   "metadata": {
    "id": "fdee6492-2dfe-44c0-be45-3c8db1492e78"
   },
   "outputs": [],
   "source": [
    "tgt_src = pd.Series([t[:-1].cpu() for t in tgt])\n",
    "tgt_src = tgt_src.apply(to_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daf3fab8-28f0-40ab-a354-8ee6d9d3d12e",
   "metadata": {
    "id": "daf3fab8-28f0-40ab-a354-8ee6d9d3d12e"
   },
   "outputs": [],
   "source": [
    "tgt_tgt = pd.Series([t[1:].cpu() for t in tgt])\n",
    "tgt_tgt = tgt_tgt.apply(to_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22d441df-e3ae-409b-bf05-abaa8645c008",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22d441df-e3ae-409b-bf05-abaa8645c008",
    "outputId": "b478f5a1-de98-419b-ff25-502180b493ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   src      50000 non-null  object\n",
      " 1   tgt_src  50000 non-null  object\n",
      " 2   tgt_tgt  50000 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "st_ = {\n",
    "    'src':src,\n",
    "    'tgt_src':tgt_src,\n",
    "    'tgt_tgt':tgt_tgt\n",
    "}\n",
    "\n",
    "src_tgt_ = pd.concat(st_, axis=1)\n",
    "print(src_tgt_.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be1768e5-a118-4fbb-b0f7-c591c6b8a2a2",
   "metadata": {
    "id": "be1768e5-a118-4fbb-b0f7-c591c6b8a2a2"
   },
   "outputs": [],
   "source": [
    "max_seq_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd41a73b-ad68-403a-8efc-2e5a2f2ca6f9",
   "metadata": {
    "id": "cd41a73b-ad68-403a-8efc-2e5a2f2ca6f9"
   },
   "outputs": [],
   "source": [
    "src_tgt = src_tgt_[(src_tgt_['src'].apply(lambda x: x.numel() <= max_seq_len))\n",
    "    & (src_tgt_['tgt_src'].apply(lambda x: x.numel() <= max_seq_len))\n",
    "    & (src_tgt_['tgt_tgt'].apply(lambda x: x.numel() <= max_seq_len))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fffc251a-b8f1-42f1-8354-13521418fbde",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fffc251a-b8f1-42f1-8354-13521418fbde",
    "outputId": "e97a65b7-3b03-4ca7-c75c-7b87e9e372bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 49988 entries, 0 to 49999\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   src      49988 non-null  object\n",
      " 1   tgt_src  49988 non-null  object\n",
      " 2   tgt_tgt  49988 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "src_tgt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1875a3cf-34cd-4a1a-99ea-8d2d519c7cda",
   "metadata": {
    "id": "1875a3cf-34cd-4a1a-99ea-8d2d519c7cda"
   },
   "outputs": [],
   "source": [
    "def pad_seq(seq):\n",
    "    seq_padded = F.pad(seq, (0, max_seq_len - len(seq)), 'constant', PAD_IDX)\n",
    "    return seq_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "117e01db-c5f8-49ba-a12d-06563502a965",
   "metadata": {
    "id": "117e01db-c5f8-49ba-a12d-06563502a965"
   },
   "outputs": [],
   "source": [
    "def apply_pad_to_df(series):\n",
    "    series = series.apply(pad_seq)\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b20cc8f3-56a8-4772-bf19-864692200ebd",
   "metadata": {
    "id": "b20cc8f3-56a8-4772-bf19-864692200ebd"
   },
   "outputs": [],
   "source": [
    "data_prepared = src_tgt.apply(apply_pad_to_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c532ee6d-98f0-4721-ae7f-a09ef5b498aa",
   "metadata": {
    "id": "c532ee6d-98f0-4721-ae7f-a09ef5b498aa"
   },
   "outputs": [],
   "source": [
    "data_length = len(data_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff4156be-80d1-4f12-b459-8d834b66a648",
   "metadata": {
    "id": "ff4156be-80d1-4f12-b459-8d834b66a648"
   },
   "outputs": [],
   "source": [
    "train_data = data_prepared[:int(0.9*data_length)]\n",
    "val_data = data_prepared[int(0.9*data_length):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7bb26-a7c2-4662-aa11-d221b6772f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e8e6066-4147-4e97-a6e1-33ba70c68ab3",
   "metadata": {
    "id": "5e8e6066-4147-4e97-a6e1-33ba70c68ab3"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    #one head of self-attention\n",
    "    def __init__(self, d_model, head_size):\n",
    "        super().__init__()\n",
    "        self.key_linear = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.query_linear = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.value_linear = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "\n",
    "        query = self.query_linear(query)\n",
    "        key = self.key_linear(key)\n",
    "        value = self.value_linear(value)\n",
    "\n",
    "        #compute attention scores\n",
    "        wei = query @ key.transpose(-2, -1) * key.shape[-1] **-0.5 #(B,T,hs) @ (B,hs,T) -> (B,T,T)\n",
    "        if mask is not None:\n",
    "            wei = wei.masked_fill(mask == 0, float('-1e9')) # (B,T,T)\n",
    "        wei = F.softmax(wei, dim=-1) #(B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "        #perform the weighted agregation of the values\n",
    "        out = wei @ value\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(d_model, head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        out = torch.cat([h(query, key, value, mask) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super().__init__()\n",
    "        head_size = d_model // n_head\n",
    "        # communication\n",
    "        self.sa = MultiHeadAttention(d_model, n_head, head_size)\n",
    "        #computation\n",
    "        self.ffwd = FeedForward(d_model)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "    def forward(self, src, mask=None):\n",
    "        x = self.ln1(src)\n",
    "        src = src + self.sa(x, x, x, mask)\n",
    "        x = self.ln2(src)\n",
    "        out = src + self.ffwd(x)\n",
    "        return out\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super().__init__()\n",
    "        head_size = d_model // n_head\n",
    "        # communication\n",
    "        self.sa1 = MultiHeadAttention(d_model, n_head, head_size)\n",
    "\n",
    "        self.sa2 = MultiHeadAttention(d_model, n_head, head_size)\n",
    "        #computation\n",
    "        self.ffwd = FeedForward(d_model)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ln3 = nn.LayerNorm(d_model)\n",
    "    def forward(self, tgt, enc_out, tgt_mask=None):\n",
    "\n",
    "        x = self.ln1(tgt)\n",
    "        tgt = tgt + self.sa1(x, x, x, tgt_mask)\n",
    "\n",
    "        x = tgt + self.sa2(self.ln2(tgt), enc_out, enc_out)\n",
    "\n",
    "        out = x + self.ffwd(self.ln3(x))\n",
    "        return out\n",
    "        \n",
    "def sin_pos_enc(seq_length, d_model):\n",
    "    position = torch.arange(seq_length).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000) / d_model))\n",
    "    pos_enc = torch.zeros(seq_length, d_model)\n",
    "    pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "    pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pos_enc\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size = 2000, d_model= 64, n_head = 8, n_layer=6):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # self.position_embedding = nn.Embedding(block_size, d_model)\n",
    "\n",
    "        position = torch.arange(block_size).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000) / d_model))\n",
    "        pos_enc = torch.zeros(block_size, d_model)\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe',pos_enc)\n",
    "        \n",
    "        # self.layers = nn.Sequential(*[EncoderLayer(d_model, n_head) for _ in range(n_layer)])\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_head)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != PAD_IDX).unsqueeze(1)\n",
    "        return src_mask.to(device)\n",
    "\n",
    "    def forward(self, src):\n",
    "        B, T = src.shape\n",
    "        src_mask = self.make_src_mask(src)\n",
    "\n",
    "        tok_emb = self.token_embedding(src)\n",
    "        # pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
    "        # x = tok_emb + pos_emb\n",
    "        x = tok_emb + self.pe[:T]\n",
    "        # x = self.layers(x, src_mask)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        output = self.ln_f(x)\n",
    "        return output.to(device)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size = 2000, d_model= 64, n_head = 8, n_layer=6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # self.position_embedding = nn.Embedding(block_size, d_model)\n",
    "        position = torch.arange(block_size).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000) / d_model))\n",
    "        pos_enc = torch.zeros(block_size, d_model)\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe',pos_enc)\n",
    "\n",
    "        # self.layers = nn.Sequential(*[DecoderLayer(d_model, n_head) for _ in range(n_layer)])\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_head)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def make_tgt_mask(self, tgt):\n",
    "        N, tgt_len = tgt.shape\n",
    "        tgt_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=device))\n",
    "        tgt_padding_mask = (tgt != PAD_IDX).unsqueeze(1)\n",
    "        tgt_mask = tgt_mask.to(torch.int32)\n",
    "        tgt_mask = tgt_mask & tgt_padding_mask\n",
    "        return tgt_mask\n",
    "\n",
    "    def forward(self, tgt, enc_out):\n",
    "        B, T = tgt.shape\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "        tok_emb = self.token_embedding(tgt)\n",
    "        # pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
    "        # x = tok_emb + pos_emb\n",
    "        x = tok_emb + self.pe[:T]\n",
    "        # x = self.layers(x, enc_out, tgt_mask)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, tgt_mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        return logits\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, enc_vocab_size, dec_vocab_size, d_model = 64, n_head=8, n_layer=6):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(enc_vocab_size, d_model, n_head, n_layer)\n",
    "        self.decoder = Decoder(dec_vocab_size, d_model, n_head, n_layer)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        enc_out = self.encoder(src)\n",
    "        out = self.decoder(tgt, enc_out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b0416664-528d-409a-b36a-9b9c0cc826ee",
   "metadata": {
    "id": "b0416664-528d-409a-b36a-9b9c0cc826ee"
   },
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# block_size = max_seq_len\n",
    "# dropout= 0.1\n",
    "# d_model = 512\n",
    "# n_head = 8\n",
    "# n_layer = 6\n",
    "# learning_rate = 3e-4\n",
    "# max_iters = 20000\n",
    "batch_size = 8\n",
    "block_size = max_seq_len\n",
    "dropout= 0.1\n",
    "d_model = 16\n",
    "n_head = 4\n",
    "n_layer = 3\n",
    "learning_rate = 1e-3\n",
    "max_iters = 1000\n",
    "warmup_steps = int(max_iters * 0.08)\n",
    "model = Transformer(vocab_src, vocab_tgt, d_model, n_head, n_layer).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e713a36-cd6c-41ab-b29e-e43a53cb0931",
   "metadata": {
    "id": "6e713a36-cd6c-41ab-b29e-e43a53cb0931"
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = [random.randint(0, len(data)-1) for _ in range(batch_size)]\n",
    "    x = torch.stack([data['src'].iloc[i] for i in ix]).to(device)\n",
    "    y_src = torch.stack([data['tgt_src'].iloc[i] for i in ix]).to(device)\n",
    "    y_tgt = torch.stack([data['tgt_tgt'].iloc[i] for i in ix]).to(device)\n",
    "    return x, y_src, y_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e2bab26-8ef6-4c84-9244-e1f5e80c8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))  # Linear warm-up\n",
    "    return max(0.0, (max_iters - current_step) / (max_iters - warmup_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c3e7d0e-2bee-41f3-838e-99bca8dccf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, learning_rate = 1e-3, max_iters=3000):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    for iter in range(max_iters):\n",
    "        xb, yb_src, yb_tgt = get_batch('train')\n",
    "        logits = model(xb, yb_src)\n",
    "        # print(logits)\n",
    "        B,T,C = logits.shape\n",
    "        logits = logits.view(B*T,C)\n",
    "        yb_tgt = yb_tgt.view(B*T)\n",
    "        loss = criterion(logits, yb_tgt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if iter%100 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"iter: {iter}, loss: {loss.item():.4f}, lr: {current_lr}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f6d78cd-1bf4-4dea-b767-daf5353f0297",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f6d78cd-1bf4-4dea-b767-daf5353f0297",
    "outputId": "817207c8-b07b-4bf8-f1fe-526bb7b52dc3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 8.0498, lr: 1.25e-05\n",
      "iter: 100, loss: 4.7974, lr: 0.0009771739130434783\n",
      "iter: 200, loss: 3.2522, lr: 0.0008684782608695653\n",
      "iter: 300, loss: 2.5951, lr: 0.0007597826086956522\n",
      "iter: 400, loss: 1.7517, lr: 0.0006510869565217391\n",
      "iter: 500, loss: 2.0149, lr: 0.0005423913043478262\n",
      "iter: 600, loss: 2.2431, lr: 0.0004336956521739131\n",
      "iter: 700, loss: 2.2962, lr: 0.00032500000000000004\n",
      "iter: 800, loss: 2.0313, lr: 0.00021630434782608695\n",
      "iter: 900, loss: 2.6324, lr: 0.00010760869565217392\n"
     ]
    }
   ],
   "source": [
    "model = train(model, learning_rate=learning_rate, max_iters=max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bade36-65d6-4521-bf7c-17d745f6aa7c",
   "metadata": {
    "id": "e6bade36-65d6-4521-bf7c-17d745f6aa7c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aebc19de-3029-49a1-9831-a501a80d3a06",
   "metadata": {
    "id": "aebc19de-3029-49a1-9831-a501a80d3a06"
   },
   "outputs": [],
   "source": [
    "def generate(model, src, max_tokens = 50):\n",
    "    model.eval()\n",
    "    tgt = torch.tensor([BOS_IDX]).to(device).unsqueeze(0).long()\n",
    "    for _ in range(max_tokens):\n",
    "        logits = model(src, tgt)\n",
    "        #focus only on the last time step\n",
    "        logits = logits[:, -1, :] #becomes (B,C)\n",
    "        #apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1) #(B,C)\n",
    "        #sample from the distribution\n",
    "        token_next = torch.multinomial(probs, num_samples=1)\n",
    "        # token_next = torch.argmax(probs)\n",
    "        #append sampled index to the running sequence\n",
    "        tgt = torch.cat((tgt, token_next), dim = 1) # (B, T+1)\n",
    "        if token_next == EOS_IDX:\n",
    "            break\n",
    "\n",
    "    return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40e3eba8-7e39-42e3-b946-125c94e38761",
   "metadata": {
    "id": "40e3eba8-7e39-42e3-b946-125c94e38761"
   },
   "outputs": [],
   "source": [
    "def gen(text, model, sp_src, sp_tgt, max_tokens = 50):\n",
    "    src = pad_seq(process_text(text, sp_src)).view(1, -1)\n",
    "    tgt = generate(model, src, max_tokens)\n",
    "    return sp_tgt.decode(tgt.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6b6cbd0-2918-4da5-befd-fe7c7dcd878c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6b6cbd0-2918-4da5-befd-fe7c7dcd878c",
    "outputId": "6363be92-8d65-4fc8-b16b-7304a39463e9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat is love?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# text='hello world'\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m res \u001b[38;5;241m=\u001b[39m gen(text, \u001b[43mmodel\u001b[49m, sp_src, sp_tgt, \u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "text='What is love?'\n",
    "# text='hello world'\n",
    "res = gen(text, model, sp_src, sp_tgt, 50)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QMavJRnaHcvQ",
   "metadata": {
    "id": "QMavJRnaHcvQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600050d9-ee8b-46e5-a12a-7310f444a03a",
   "metadata": {
    "id": "600050d9-ee8b-46e5-a12a-7310f444a03a"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "path = f\"./transformer_50k_v1_b{batch_size}_t{block_size}_dm{d_model}_nh{n_head}_nl{n_layer}_lr{learning_rate}_mi{max_iters}.pt\"\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3807a8a-da60-462c-a35d-db1eb60c4cf7",
   "metadata": {
    "id": "a3807a8a-da60-462c-a35d-db1eb60c4cf7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc3bf61-2d60-4ada-976d-da48c389e93d",
   "metadata": {
    "id": "8bc3bf61-2d60-4ada-976d-da48c389e93d"
   },
   "outputs": [],
   "source": [
    "path_test= f\"./drive/MyDrive/Colab Notebooks/transformer_test.pt\"\n",
    "torch.save(model_test_save.state_dict(), path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1cab3-9e98-4d3c-b22d-72f283a17114",
   "metadata": {
    "id": "16f1cab3-9e98-4d3c-b22d-72f283a17114"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f4fcee-49cd-4393-802c-4c1a5913fbbf",
   "metadata": {
    "id": "05f4fcee-49cd-4393-802c-4c1a5913fbbf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5422793d-9100-4f43-b1c3-71d9143878a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5422793d-9100-4f43-b1c3-71d9143878a7",
    "outputId": "ebb6b60d-0f3f-445b-b203-a12a388f04f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load model\n",
    "batch_size = 64\n",
    "block_size = max_seq_len\n",
    "dropout= 0.1\n",
    "d_model = 512\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "learning_rate = 3e-4\n",
    "max_iters = 20000\n",
    "# path_load= f\"./drive/MyDrive/Colab Notebooks/transformer_50k_v1_b{batch_size}_t{block_size}_dm{d_model}_nh{n_head}_nl{n_layer}_lr{learning_rate}_mi{max_iters}.pt\"\n",
    "# path_load = './models/transformer_50k_v1_3_3_b64_t100_dm512_nh4_nl4_lr0.0003_wms1600_mi20000.pt'\n",
    "path_load = './models/transformer_50k_v1_2_b64_t100_dm512_nh4_nl4_lr0.0003_wms1600_mi20000.pt'\n",
    "model_loaded = Transformer(vocab_src, vocab_tgt, d_model, n_head, n_layer).to(device)\n",
    "model_loaded.load_state_dict(torch.load(path_load, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0ace07e-11da-4f38-a1d9-3a28b86e4b38",
   "metadata": {
    "id": "c0ace07e-11da-4f38-a1d9-3a28b86e4b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Что же война?']\n"
     ]
    }
   ],
   "source": [
    "text='What is war?'\n",
    "res = gen(text, model_loaded, sp_src, sp_tgt, 50)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d9cf44-fe9f-4b27-a4f7-52bf3902cce3",
   "metadata": {
    "id": "35d9cf44-fe9f-4b27-a4f7-52bf3902cce3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c05c6d1-5b59-4a12-a2b4-58f5af5662aa",
   "metadata": {
    "id": "8c05c6d1-5b59-4a12-a2b4-58f5af5662aa"
   },
   "outputs": [],
   "source": [
    "def generate2(model, src, max_tokens = 50, batch_size=8):\n",
    "    model.eval()\n",
    "    # tgt = torch.tensor([BOS_IDX]).unsqueeze(0).long().to(device)\n",
    "    tgt = torch.full((batch_size,1), BOS_IDX).to(device)\n",
    "    for _ in range(max_tokens):\n",
    "        logits = model(src, tgt)\n",
    "        #focus only on the last time step\n",
    "        logits = logits[:, -1, :] #becomes (B,C)\n",
    "        #apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1) #(B,C)\n",
    "        #sample from the distribution\n",
    "        token_next = torch.multinomial(probs, num_samples=1)\n",
    "        # token_next = torch.argmax(probs)\n",
    "        #append sampled index to the running sequence\n",
    "        tgt = torch.cat((tgt, token_next), dim = 1) # (B, T+1)\n",
    "    return tgt\n",
    "def gen2(text, model, sp_src, sp_tgt, max_tokens = 50, batch_size = 8):\n",
    "    src = pad_seq(process_text(text, sp_src))\n",
    "    src_batches = torch.stack([src for _ in range(batch_size)]).to(device)\n",
    "    tgt = generate2(model, src_batches, max_tokens, batch_size)\n",
    "    for row in tgt:\n",
    "        idx = torch.where(row == EOS_IDX)[0]\n",
    "        if len(idx) > 0:\n",
    "            row[idx[0] + 1:] = PAD_IDX\n",
    "    return sp_tgt.decode(tgt.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c8f2b9d-e480-4e8d-8cd8-47de9614eea7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c8f2b9d-e480-4e8d-8cd8-47de9614eea7",
    "outputId": "e2c91140-fe0e-44f4-e213-53e56a50ee75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На тренировачных данных: \n",
      "Английский:  We have an average of 6% of such errors.\n",
      "Действительный перевод:  У нас средний показатель таких ошибок составляет 6%.\n",
      "Перевод модели:\n",
      "1) У нас средний показатель таких ошибок составляетмарПредо. послать.\n",
      "2) У нас средний показатель таких ошибок составляет 6%.\n",
      "3) У нас средний показатель такого ошибок составляет 6%.\n",
      "4) У нас средний показатель таких ошибок составляет 6% этой ошибки.\n",
      "5) У нас средний показатель таких ошибок составляет 6%.\n",
      "6) У нас средний показатель таких ошибок составляет 6 исповеИдея.\n",
      "7) У нас средний показатель таких ошибок составляет 6% этой статьи.\n",
      "8) У нас средний показатель таких ошибок составляет 6 процентов.\n",
      "9) У нас средний показатель таких ошибок составляет 6%.\n",
      "10) У нас средний показатель таких ошибок известна.\n",
      "11) У нас средний показатель таких ошибок составляет 6%.\n",
      "12) У нас средний показатель таких ошибок периода ждут подобных ошибок.\n",
      "13) У нас средний показатель таких ошибок составляет 6%центра.\n",
      "14) У нас средний показатель спросом муниципалитетов 6 процентов породы.\n",
      "15) У нас средний показатель таких ошибок составляет 6% таких ошибок.\n",
      "16) У нас средний обычных футбольных поя 6% таких ошибок. пота.\n"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "text = data['en'].iloc[k]\n",
    "print('На тренировачных данных: ')\n",
    "print('Английский: ', text)\n",
    "print('Действительный перевод: ', data['ru'].iloc[k])\n",
    "print('Перевод модели:')\n",
    "res = gen2(text, model_loaded, sp_src, sp_tgt, 50, batch_size=16)\n",
    "for i in range(len(res)):\n",
    "    print(f'{i+1})',res[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9778fce8-fec2-4674-8a38-62ea4c7999eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На данных проверки: \n",
      "Английский:  And in winter - because of shoes, a lot of complaints about the quality and reagents that spoil footwear - we can't help here, unfortunately.\n",
      "Действительный перевод:  А в зимний период - из ‑ за обуви и множества жалоб на ее качество и вредные вещества, которые портят обувь, - мы здесь, увы, ничем помочь не можем.\n",
      "Перевод модели:\n",
      "1) департамент- знаний, потому что обувиaster + много вечера - это много отображает своих трассах гибкого отметил - нельзя удобнее нельзя, увы произойти.\n",
      "2) А за \" экскурсии\" - потому что с водители много плохих и их ребенок, которые пор заметил тудавешивает в том, что ониторах - не могут помочьтельный.\n",
      "3) А у Зимой - ведь подразделений, много�ходные жалобы и жидкостей, которые пор произведения, гиподите должныор покраститься - мы не можем понять, увы.\n",
      "4) серьёзно называют выставок, потому что обуви многония жалоб обозначающие ваших пространств и обработки ее невозможно, которые порвать - мы можем сержусь.\n",
      "5) А передний период - волорилВместе к качеству разговора о качества молодежи и закрылзовывать ее,� дисципли дебют - мы не можемм.\n",
      "6) А у закрытых зимы - потому что очень почувствовала много волноваться о качества и их устойчивость, которые ви осознают объема городов - взаимодей документ, к сожалению предприятий заключается.\n",
      "7) А у зимой - потому что садигут много сравнения о качествах и протоляемых - это нешихся, которые портят чешу,ooperской.\n",
      "8) А у зимний - потому что обуви по внешнему карты и генетики мужских утверждений, что поромяг ни овцы - можно безболез млрд, увы.\n",
      "9) А у зимнем - потому что обуви профили по качествус и негодование, которые портят вид что не можем злоумышленники - мы не можем плохом.\n",
      "10) А в зимний период - потому чтоеся молоде вызван к качество и разделов, которые порпалRE Срок - мы можем собрать нас здесь, к сожалению.\n",
      "11) Каome - потому что обуви, тенков много долгих ивят теми прически, которые порстю - пересекаются здесь слабые стороны.\n",
      "12) А передний - потомусайт о обувьниях смартфону беспоко угла и глубокий вдохрядкиводы - мы сохранять их Corpop или рентгестилад.\n",
      "13) А у зимний - ведьпомним, много сравнении многопособных детей и туманных безрассуд макро не могут помочь, что-нибудьбрегастет.\n",
      "14) А у зимний период - из-за йоги много жалобы и рожденных тканей, которые катали статьи, эле звезды - мы имеем не можем волшебников, к сожалению.\n",
      "15) настоящее время зимнего - потому что обуви,� многорных кар меда млрд и их выступления неизменным, которые портят здесь знающие - мы можем не можем им взаиможественные.\n",
      "16) Сроз, потому что обуви - это много качество много китов и вершины, которыегноют травы - мы можемчиная с иннова превратитьНесколько, увы.\n"
     ]
    }
   ],
   "source": [
    "k = 48000\n",
    "text = data['en'].iloc[k]\n",
    "print('На данных проверки: ')\n",
    "print('Английский: ', text)\n",
    "print('Действительный перевод: ', data['ru'].iloc[k])\n",
    "print('Перевод модели:')\n",
    "res = gen2(text, model_loaded, sp_src, sp_tgt, 50, batch_size=16)\n",
    "for i in range(len(res)):\n",
    "    print(f'{i+1})',res[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "avqKXHgqx2q3",
   "metadata": {
    "id": "avqKXHgqx2q3"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, max_iters=50):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for iter in range(max_iters):\n",
    "      xb, yb_src, yb_tgt = get_batch('val')\n",
    "      logits = model(xb, yb_src)\n",
    "      B,T,C = logits.shape\n",
    "      logits = logits.view(B*T,C)\n",
    "      yb_tgt = yb_tgt.view(B*T)\n",
    "      loss = criterion(logits, yb_tgt)\n",
    "      print(f'iter: {iter}, loss:{loss.item():.4f}')\n",
    "      val_loss += loss.item()\n",
    "    return val_loss/max_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "MbGR-0sex2hM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MbGR-0sex2hM",
    "outputId": "c0d3aa3e-2284-484c-b296-af8da12cc98c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss:1.0090\n",
      "iter: 1, loss:1.1734\n",
      "iter: 2, loss:1.0213\n",
      "iter: 3, loss:1.1277\n",
      "iter: 4, loss:0.9835\n",
      "iter: 5, loss:1.0779\n",
      "iter: 6, loss:1.1955\n",
      "iter: 7, loss:1.1396\n",
      "iter: 8, loss:0.9618\n",
      "iter: 9, loss:1.0927\n",
      "iter: 10, loss:1.0368\n",
      "iter: 11, loss:1.1884\n",
      "iter: 12, loss:1.2688\n",
      "iter: 13, loss:1.1463\n",
      "iter: 14, loss:1.2642\n",
      "iter: 15, loss:1.0025\n",
      "iter: 16, loss:1.1225\n",
      "iter: 17, loss:1.1225\n",
      "iter: 18, loss:1.0859\n",
      "iter: 19, loss:1.2552\n",
      "iter: 20, loss:1.0194\n",
      "iter: 21, loss:1.1958\n",
      "iter: 22, loss:1.2199\n",
      "iter: 23, loss:1.2517\n",
      "iter: 24, loss:1.1477\n",
      "iter: 25, loss:0.9847\n",
      "iter: 26, loss:1.0515\n",
      "iter: 27, loss:1.0184\n",
      "iter: 28, loss:1.0359\n",
      "iter: 29, loss:1.0765\n",
      "iter: 30, loss:0.9663\n",
      "iter: 31, loss:1.1760\n",
      "iter: 32, loss:0.9498\n",
      "iter: 33, loss:1.0033\n",
      "iter: 34, loss:1.1084\n",
      "iter: 35, loss:1.2245\n",
      "iter: 36, loss:1.2702\n",
      "iter: 37, loss:1.1871\n",
      "iter: 38, loss:1.0310\n",
      "iter: 39, loss:1.2082\n",
      "iter: 40, loss:0.9992\n",
      "iter: 41, loss:1.1739\n",
      "iter: 42, loss:1.0968\n",
      "iter: 43, loss:1.0094\n",
      "iter: 44, loss:0.9874\n",
      "iter: 45, loss:0.9997\n",
      "iter: 46, loss:1.1233\n",
      "iter: 47, loss:1.2442\n",
      "iter: 48, loss:1.1542\n",
      "iter: 49, loss:1.0366\n",
      "1.1045374953746796\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model_loaded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "N6YB-DW2FE44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6YB-DW2FE44",
    "outputId": "49ffa501-28d1-4b66-9601-267daeaa5aa7"
   },
   "outputs": [],
   "source": [
    "def evaluate_training_data(model, max_iters=50):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing = 0.05)\n",
    "    for iter in range(max_iters):\n",
    "      xb, yb_src, yb_tgt = get_batch('train')\n",
    "      logits = model(xb, yb_src)\n",
    "      B,T,C = logits.shape\n",
    "      logits = logits.view(B*T,C)\n",
    "      yb_tgt = yb_tgt.view(B*T)\n",
    "      loss = criterion(logits, yb_tgt)\n",
    "      print(f'iter: {iter}, loss:{loss.item():.4f}')\n",
    "      val_loss += loss.item()\n",
    "    return val_loss/max_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7gab9CwUF2H4",
   "metadata": {
    "id": "7gab9CwUF2H4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss:0.7476\n",
      "iter: 1, loss:0.7484\n",
      "iter: 2, loss:0.7487\n",
      "iter: 3, loss:0.7448\n",
      "iter: 4, loss:0.7451\n",
      "iter: 5, loss:0.7376\n",
      "iter: 6, loss:0.7423\n",
      "iter: 7, loss:0.7484\n",
      "iter: 8, loss:0.7444\n",
      "iter: 9, loss:0.7425\n",
      "iter: 10, loss:0.7469\n",
      "iter: 11, loss:0.7392\n",
      "iter: 12, loss:0.7434\n",
      "iter: 13, loss:0.7438\n",
      "iter: 14, loss:0.7458\n",
      "iter: 15, loss:0.7446\n",
      "iter: 16, loss:0.7428\n",
      "iter: 17, loss:0.7456\n",
      "iter: 18, loss:0.7437\n",
      "iter: 19, loss:0.7465\n",
      "iter: 20, loss:0.7426\n",
      "iter: 21, loss:0.7399\n",
      "iter: 22, loss:0.7382\n",
      "iter: 23, loss:0.7443\n",
      "iter: 24, loss:0.7433\n",
      "iter: 25, loss:0.7462\n",
      "iter: 26, loss:0.7427\n",
      "iter: 27, loss:0.7408\n",
      "iter: 28, loss:0.7488\n",
      "iter: 29, loss:0.7399\n",
      "iter: 30, loss:0.7407\n",
      "iter: 31, loss:0.7465\n",
      "iter: 32, loss:0.7417\n",
      "iter: 33, loss:0.7452\n",
      "iter: 34, loss:0.7412\n",
      "iter: 35, loss:0.7400\n",
      "iter: 36, loss:0.7436\n",
      "iter: 37, loss:0.7464\n",
      "iter: 38, loss:0.7475\n",
      "iter: 39, loss:0.7517\n",
      "iter: 40, loss:0.7455\n",
      "iter: 41, loss:0.7422\n",
      "iter: 42, loss:0.7442\n",
      "iter: 43, loss:0.7412\n",
      "iter: 44, loss:0.7401\n",
      "iter: 45, loss:0.7426\n",
      "iter: 46, loss:0.7441\n",
      "iter: 47, loss:0.7454\n",
      "iter: 48, loss:0.7450\n",
      "iter: 49, loss:0.7509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7440951931476593"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_training_data(model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f857a66-e914-4248-a0ce-e65257b4af57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "446c06ee-2475-488c-9b92-79e3bd38b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleu score\n",
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f42b5be-fe19-4bfe-abd3-45717ace8933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_corpus(size = 20):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(size):\n",
    "        k = random.randint(0, int(0.9*len(data_prepared)))\n",
    "        xs.append(data.iloc[k]['en'])\n",
    "        ys.append(data.iloc[k]['ru'])\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "493e8ec5-9f95-4b74-ae47-b87e1a12c7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 100.0000\n"
     ]
    }
   ],
   "source": [
    "# Reference translations (should be a list of strings, not tokenized)\n",
    "references = [[\"The cat is on the mat.\" , \"There is a cat on the mat.\"]]\n",
    "\n",
    "# Candidate translation (Model's output)\n",
    "candidate = [\"The cat is on the mat dermo hui\"]\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu = sacrebleu.corpus_bleu(candidate, references)\n",
    "print(f\"BLEU Score: {bleu.score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "34d273fb-4075-4d22-9857-6d4065ab581b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 2.247346032110286, 2.6757454896417534, 2.3629035975324633, 1.5843969409738214, 0.0, 2.0705706652424007, 2.8316557261689033, 0.9584157964125064, 2.159701133933343, 1.8815557141800423, 1.3862040232457764, 2.0540268312306345, 0.0, 3.0890553181566975, 4.767707020457095, 1.387819277861591, 4.935157841536379, 4.196114906296549, 1.7392453207295933]\n"
     ]
    }
   ],
   "source": [
    "def get_bleu_score(model):\n",
    "    x, y = get_random_corpus()\n",
    "    candidates = []\n",
    "    bleus = []\n",
    "    for i in range(len(x)):\n",
    "        res = gen(x[i], model, sp_src, sp_tgt, 50)\n",
    "        bleu = sacrebleu.corpus_bleu(res, [[y[i]]])\n",
    "        bleus.append(bleu.score)\n",
    "    print(bleus)\n",
    "    return bleaus\n",
    "bleaus = get_bleu_score(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a7049-26e8-4557-8654-95af3e567d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
